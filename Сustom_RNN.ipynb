{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fUkc3q1sbAU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "import math\n",
        "\n",
        "class myRNN:\n",
        "    # будем часто использовать np.dot для скалярного перемножения\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        # нормальзирую веса, домножив на 0.1\n",
        "        self.Wxh = randn(hidden_size, input_size) * 0.1\n",
        "        self.Whh = randn(hidden_size, hidden_size) * 0.1\n",
        "        self.Why = randn(output_size, hidden_size) * 0.1\n",
        "\n",
        "        # Это векторы смещения\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    # forward прохода нейронной сети на основании информации и фотографии\n",
        "    def forward(self, inputs):\n",
        "        h = np.zeros((self.Whh.shape[0], 1))\n",
        "        self.last_hs = { 0: h }\n",
        "        last_inputs = inputs\n",
        "\n",
        "        for i, x in enumerate(inputs):\n",
        "            # выписываем формулу для вычисления следующего скрытого состояния с фотографии\n",
        "            h = np.tanh(np.dot(self.Wxh, x.reshape(-1, 1)) + np.dot(self.Whh, h) + self.bh)\n",
        "            self.last_hs[i+1] = h_nуw\n",
        "            # Обновляем h для следующей итерации\n",
        "            h = h_nyw\n",
        "\n",
        "\n",
        "        y = np.dot(self.Why, h) + self.by\n",
        "        return y, h\n",
        "\n",
        "\n",
        "    # backprop обратного распространения ошибки\n",
        "    def backprop(self, d_y, learn_rate):\n",
        "        d_Why = np.dot(d_y, self.last_hs[len(self.last_hs) - 1].T) # необходимо также транспонировать, чтобы премножение матриц произошло корректно\n",
        "        d_by = d_y\n",
        "        last_gradient = np.dot(self.Why.T, d_y) # градиент скрытого состояния, опять транспонируем\n",
        "\n",
        "        tan = last_gradient * (1 - self.last_hs[len(self.last_hs) - 1] ** 2) # считает производную тангенса\n",
        "\n",
        "        # Градиенты для Whh, Wxh, и bh\n",
        "        d_Wxh = np.dot(tan, inputs[-1].reshape(1, -1)) # получившая производная с картинки, если нет предыдущего состояния\n",
        "        d_Whh = np.dot(tan, self.last_hs[len(self.last_hs) - 2].T) if len(self.last_hs) > 1 else np.zeros_like(self.Whh) # если есть предыдущее состояние, то подсчитываем по такой формуле\n",
        "        d_bh = tan\n",
        "\n",
        "        self.Wxh -= learn_rate * d_Wxh\n",
        "        self.Whh -= learn_rate * d_Whh\n",
        "        self.Why -= learn_rate * d_Why\n",
        "        self.bh -= learn_rate * d_bh\n",
        "        self.by -= learn_rate * d_by\n"
      ]
    }
  ]
}